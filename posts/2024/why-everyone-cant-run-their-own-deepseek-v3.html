<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="/static/htmx.min.js"></script>
    <script src="/static/head-support.js"></script>

    <title hx-preserve="true">Why running DeepSeek-V3 locally isn't for everyone</title>
    <meta property="og:title" content="Why running DeepSeek-V3 locally isn't for everyone" hx-preserve="true">
    <meta property="og:description" content="A deep-dive on DeepSeek-V3, how we got here - and whether it's even feasible to run it yourself" hx-preserve="true">
    <meta property="article:published_time" content="2024-12-31" hx-preserve="true">
    <meta property="article:modified_time" content="2025-01-05" hx-preserve="true">
    <meta property="article:tag" content="OpenAI" hx-preserve="true">
    <meta property="article:tag" content="Mistral" hx-preserve="true">
    <meta property="article:tag" content="Meta" hx-preserve="true">
    <meta property="article:tag" content="DeepSeek AI" hx-preserve="true">
    <meta property="article:tag" content="DeepSeek-V3" hx-preserve="true">
    <meta property="article:tag" content="vLLM" hx-preserve="true">
    <meta property="article:tag" content="Local LLM" hx-preserve="true">
    <meta name="author" content="Joe Lamming" hx-preserve="true">
    <link rel="author" href="/" hx-preserve="true">
</head>

<body hx-ext="head-support">
    <div hx-get="/components/head-base.html" hx-trigger="load" hx-swap="none"></div>
    <div class="site-wrapper">
        <div id="header-container" hx-get="/components/header.html" hx-trigger="load" hx-swap="innerHTML">
            <header class="header-placeholder">
                <div class="htmx-indicator">
                    <h1 style="visibility: hidden">Title hasn't loaded yet...</h1>
                    <h2 style="visibility: hidden">Subtitle hasn't loaded yet...</h2>
                </div>
            </header>
        </div>
        <div id="nav-container" hx-get="/components/navigation.html" hx-trigger="load" hx-swap="innerHTML">
            <div class="htmx-indicator">Navigation hasn't loaded yet...</div>
        </div>
        <main id="main-content">
            <div class="htmx-indicator">
                <div class="loading-placeholder">
                    <h1>Loading post...</h1>
                    <div class="shimmer-line"></div>
                    <div class="shimmer-line"></div>
                </div>
            </div>
            <article class="blog-post">
                <header class="post-header">
                    <h1>Why running DeepSeek-V3 locally isn't for everyone</h1>
                    <div class="post-meta">
                        <div class="post-date">
                            <svg class="icon" viewBox="0 0 24 24" width="16" height="16">
                                <path fill="currentColor"
                                    d="M19,4H17V3a1,1,0,0,0-2,0V4H9V3A1,1,0,0,0,7,3V4H5A2,2,0,0,0,3,6V20a2,2,0,0,0,2,2H19a2,2,0,0,0,2-2V6A2,2,0,0,0,19,4ZM19,20H5V9H19Z" />
                            </svg>
                            <time datetime="2024-12-31">31 Dec 2024</time>
                        </div>

                        <div class="post-updated">
                            <svg class="icon" viewBox="0 0 24 24" width="16" height="16">
                                <path fill="currentColor"
                                    d="M21,11a1,1,0,0,0-1,1,8.05,8.05,0,1,1-2.22-5.5L16,8.32a1,1,0,0,0,1.41,1.41l4.35-4.35a1,1,0,0,0,0-1.41L17.41-.38A1,1,0,0,0,16,1L17.77,2.8A10,10,0,1,0,22,12,1,1,0,0,0,21,11Z" />
                            </svg>
                            <span><time datetime="2025-01-05">05 Jan 2025</time></span>
                        </div>

                    </div>
                    <div class="tags">
                        <span class="tag">OpenAI</span>
                        <span class="tag">Mistral</span>
                        <span class="tag">Meta</span>
                        <span class="tag">DeepSeek AI</span>
                        <span class="tag">DeepSeek-V3</span>
                        <span class="tag">vLLM</span>
                        <span class="tag">Local LLM</span>
                    </div>
                </header>

                <div class="post-content">
                    <h3>Concurrency is key to cost-effectiveness</h3>
                    <div class="post-image-wrapper">
                        <img src="/images/concurrencyCover_768.png" alt="Concurrency Cover" loading="lazy" width="768"
                            height="692" onload="this.classList.add('loaded')" class="post-image">
                    </div>
                    <h3>Quant fund follows 236B MoE monster with 671B behemoth</h3>
                    <p>The quantitative hedge fund High-Flyer Capital Management and its DeepSeek AI lab just <a
                            href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">released</a> its
                        biggest and most compelling LLM, DeepSeek-V3. Founded in 2015 and with offices in Hong Kong and
                        Hangzou, an interesting <a
                            href="https://www.ft.com/content/357f3c68-b866-4c2e-b678-0d075051a260">article</a> from the
                        FT back in June describes how the firm grappled with <a
                            href="https://www.reuters.com/business/finance/chinas-quant-funds-suffer-deep-losses-amid-crackdown-2024-07-24/#:~:text=China%27s%20%22quant%20quake%22%2C%20and%20stricter%20oversight%20over%20trading%20practices">crackdowns</a>
                        on high-frequency traders – and how their 10,000 NVIDIA A100 processors, acquired before export
                        bans on China went into effect, have been rather fortuitous. Alibaba, Baidu and ByteDance have
                        been in a price war with their LLM APIs, intensified further after the 236B mixture-of-experts
                        (<a href="https://arxiv.org/abs/1701.06538">MoE</a>) DeepSeek-V2 <a
                            href="https://github.com/deepseek-ai/DeepSeek-V2">launched</a> in May. So why release
                        DeepSeek-V3 on Boxing Day 2024? What's their strategy here?</p>
                    <p>The following is a bit of an essay. Kind of a way for me to get my head around what's going on.
                        From a technical perspective – the models, the data, the scaffolding – alongside the broader
                        impact on enterprise, government, individuals. Not just what's happening in Asia – but with the
                        AI labs here in the western world. The ones I've noticed myself talking most about over the past
                        couple of years are OpenAI (Microsoft), Anthropic (AWS), Meta, Google DeepMind and Mistral. The
                        top <a href="https://en.wikipedia.org/wiki/Big_Tech">four</a> of these labs are expected to have
                        cumulatively sunk <a
                            href="https://www.ft.com/content/dc5b40cb-e446-4e2b-9faf-2d54720b33d8">$209bn</a> in CAPEX
                        in 2024, largely on GPUs from NVIDIA or on <a
                            href="https://www.theregister.com/2024/12/03/amazon_ai_chip/">home-grown</a> designs. Not to
                        mention billions more on <a
                            href="https://www.theverge.com/2024/12/5/24313909/openai-content-deal-toms-guide-future#:~:text=This%20adds%20to%20the%20string%20of%20content%20licensing%20agreements%20OpenAI%20has%20made%20in%20recent%20months">data</a>
                        and <a href="https://www.ft.com/content/95eca7ee-41e7-4106-a746-34f8383b7d71">talent</a>.</p>
                    <p>But in Asia, we're seeing these gigantic and <em>strong</em> LLMs trained on a much
                        smaller budget and then... open-sourced? DeepSeek's V3 <a
                            href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">paper</a>
                        details a few breakthroughs (at least as far as we know. Most other labs keep their tooling
                        under wraps) applied to its latest large-scale training run like mixed-precision FP8 training,
                        better load
                        balancing (critical for MoE), multi-token prediction (also useful for inference) and even their
                        OpenAI o1/<a
                            href="https://joelamming.com/posts/2024/openai-o3-changes-the-narrative.html">o3</a>-inspired
                        knowledge distillation post-training from <a
                            href="https://api-docs.deepseek.com/news/news1120">DeepSeek-R1</a>.</p>
                    <p>But what's most surprising here is that this <a
                            href="https://platform.openai.com/docs/models#gpt-4o">GPT-4o</a> and Claude 3.5 Sonnet (<a
                            href="https://www.anthropic.com/news/3-5-models-and-computer-use">10-2024</a>) class model
                        was
                        pre-trained on 14.8 trillion tokens, similar to <a
                            href="https://arxiv.org/pdf/2407.21783">Llama-3.1-405B</a>, apparently only requiring <a
                            href="https://github.com/deepseek-ai/DeepSeek-V3#:~:text=V3%20requires%20only-,2.788M%20H800,-GPU%20hours%20for">2.8
                            million</a> H800 GPU hours. Compare that to Llama-3.1-405B's <a
                            href="https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct#:~:text=Llama%203.1%20405B-,30.84M,-700">31
                            million</a> H100 GPU hours. Yes, DeepSeek-V3 has 66% more trainable parameters than Meta's
                        girthiest Llama – making it more <a href="https://arxiv.org/pdf/2001.08361">sample-efficient</a>
                        – but they also did it on the <a
                            href="https://www.reuters.com/technology/biden-cut-china-off-more-nvidia-chips-expand-curbs-more-countries-2023-10-17/#:~:text=ability%20to%20communicate%20with%20other%20chips">restrictions</a>-compliant
                        NVIDIA <a
                            href="https://chaoqing-i.com/upload/20231128/NVIDIA%20H800%20GPU%20Datasheet.pdf">H800</a>
                        instead of the coveted <a
                            href="https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet">H100</a>.
                        Both SXM versions of the cards offer 80GB VRAM with 3.35TB/s bandwidth and both deliver around
                        4TFLOPS and 1TFLOP of FP8 and FP32 performance, respectively. Where they differ is with the
                        interconnect between cards, essential for training models with hundreds of billions of
                        parameters – with the H100 delivering 900GB/s via <a
                            href="https://www.nvidia.com/en-gb/design-visualization/nvlink-bridges/">NVLink</a> and the
                        H800 capped at <em>just</em> 400GB/s.</p>
                    <p>This didn't stop DeepSeek. Their paper proposed a "DualPipe algorithm for efficient pipeline
                        parallelism", allowing their 2048 H800s to reduce pipeline bottlenecks and minimise
                        communication between GPUs via NVLink and between nodes via <a
                            href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> during training. Basically,
                        DeepSeek have delivered a frontier model on technically inferior hardware. And they did at at a
                        <em>much</em> lower cost. They
                        estimated the retail rate to rent an H800 at $2/hr. This checks out, considering eight A100s
                        apparently <a
                            href="https://arstechnica.com/information-technology/2024/09/nvidias-ai-chips-are-cheaper-to-rent-in-china-than-us/#:~:text=%246%20an%20hour%20to%20use%20a%20server%20with%20eight%20Nvidia%20A100%20processors">cost</a>
                        around $6/hour in China. This means the compute spend on training for 2.8
                        million GPU hours for DeepSeek-V3 would be around $5.6mn. Quite a feat, although like most
                        frontier AI labs they didn't talk about staff costs or compute resources consumed in preparation
                        for the final run. They did note that throughout the final run of pre-training and
                        post-training, the team "did not experience any irrecoverable loss spikes or perform any
                        rollbacks".
                    </p>
                    <p>Exciting for anyone who isn't operating at the scale of OpenAI or Anthropic. Assuming a team has
                        some experience with training large models, has access to vast amounts of RLHF/preference data
                        (difficult) some high-quality web-scale datasets (even more difficult, despite efforts from the
                        <a href="https://huggingface.co/allenai">Allen Institute</a>) and is quick to implement the
                        tricks shared in frontier AI research – a start-up with a few million in funding could
                        conceivably replicate DeepSeek-V3 from the ground up.
                    </p>
                    <p>But there's a lot more to this story than a frontier-class AI model being trained on what could
                        be the R&amp;D budget of a medium-sized enterprise. To repurpose a phrase popularised by <a
                            href="https://x.com/MKBHD/status/659455520311529472">MKBHD</a> back in 2015: <em>Small LLMs
                            are getting good. And giant LLMs are getting cheap</em>.</p>
                    <h3>Language models have been scaling <strong>up</strong> for years</h3>
                    <p>The Transformer model kicked off the language modelling race in 2017 with that famous <a
                            href="https://arxiv.org/abs/1706.03762">paper</a> everyone and their goldfish have long
                        cited in daily conversation. <a href="https://arxiv.org/abs/1810.04805">BERT</a> followed in
                        2018, alongside OpenAI's very first <em>generative pre-trained transformer</em> (<a
                            href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>).
                        The latter announced the results of an even larger model in 2019, GPT-2, albeit deciding to <a
                            href="https://github.com/openai/gpt-2/blob/master/download_model.py">release</a> the weights
                        to a smaller variant, rather than the full 1.5-billion parameter version due to "<a
                            href="https://openai.com/index/better-language-models/#:~:text=we%20are%20not%20releasing%20the%20trained%20model">concerns</a>
                        about malicious applications of the technology". They followed the same playbook with the
                        monster 175-billion parameter GPT-3 in <a href="https://arxiv.org/pdf/2005.14165">2020</a>,
                        providing access to it via <a
                            href="https://openai.com/index/openai-api/#:~:text=Today%20the%20API%20runs%20models%20with%20weights%20from%20the%C2%A0GPT%2D3">API</a>
                        only.</p>
                    <p>GPT-3 in particular caused a splash in the AI community, most immediately recognising the utility
                        of programming automated systems in natural language. I particularly liked a YouTube <a
                            href="https://www.youtube.com/watch?v=_8yVOC4ciXc">video</a> released in the wake of its
                        release, sharing it with a few friends and colleagues – it really boiled how to interact with it
                        (completion only, no instruction-tuned versions existed back then) and what it could do (mostly
                        just poems and basic arithmetic). OpenAI sold access to the full-fat Davinci 175B model via the
                        API for $60 per million tokens generated, with most commercial use cases around categorisation,
                        search reranking and very early forms of <a href="https://arxiv.org/pdf/2005.11401">RAG</a>.</p>
                    <p>Well-funded AI labs at the time continued churning out new families of models, with DeepMind's <a
                            href="https://deepmind.google/discover/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval/">Gopher</a>
                        in 2021 and Meta AI's <em>open pre-training transformer language models</em> (<a
                            href="https://arxiv.org/pdf/2205.01068">OPT</a>) and <a
                            href="https://arxiv.org/pdf/2204.06745">GPT-NeoX</a> from EleutherAI (noteworthy for
                        datasets like <a href="https://arxiv.org/pdf/2101.00027">The Pile</a> and <a
                            href="https://arxiv.org/pdf/2111.02114">LAION-400M</a>) in 2022. OpenAI's <a
                            href="https://arxiv.org/pdf/2203.02155">InstructGPT</a> in March 2022 really brought home
                        the idea of more helpful LLMs, relying less on users providing examples for the model to
                        "complete" the next example (few-shot learners) but instead allowing the user to simply describe
                        the task and have the model complete the task (zero-shot). The original GPT-3 is basically
                        autocomplete, while InstructGPT gets us closer to something resembling a chatbot.</p>
                    <h3>GPT-3.5 (aka ChatGPT) was the first <em>decent</em> public-facing chatbot</h3>
                    <p>But proper chatbots – rather than keyword regurgitators like <a
                            href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a> – capable of engaging, turn-by-turn
                        conversation, didn't really enter the <em>mainstream</em> public eye until OpenAI's November
                        2022 release of its
                        consumer-facing ChatGPT. While the AI community saw plenty of examples of chat-tuned LLMs – such
                        as Google's <a href="https://arxiv.org/pdf/2001.09977">Meena-2.6B</a> all the way back in 2020
                        and <a href="https://blog.google/technology/ai/lamda/">LaMDA</a> in 2021 (which did actually
                        make <a
                            href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">headlines</a>
                        in June 2022) – OpenAI's first foray into a truly mainstream product meant <em>everybody</em>
                        started talking about it. I have fond memories of sitting around the dinner table on Christmas
                        Day in 2022 as my relatives read out party cracker riddles – and bringing them to stunned
                        silence after reading out the perfect solution generated by GPT-3.5.</p>
                    <p>Everyone remembers what ensued. January 2023 was intense, with Microsoft announcing a massive <a
                            href="https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/">extension</a>
                        to its pre-existing 2019 <a
                            href="https://news.microsoft.com/2019/07/22/openai-forms-exclusive-computing-partnership-with-microsoft-to-build-new-azure-ai-supercomputing-technologies/">$1bn</a>
                        and 2021 $1bn (exact details on 2021 round are weirdly sparse) investment in OpenAI –
                        approximately $10bn, <a
                            href="https://www.semafor.com/article/11/18/2023/openai-has-received-just-a-fraction-of-microsofts-10-billion-investment#:~:text=while%20a%20significant%20portion%20of%20the%20funding%2C%20divided%20into%20tranches%2C%20is%20in%20the%20form%20of%20cloud%20compute%20purchases%20instead%20of%20cash">largely</a>
                        in Azure credits and access to its centralised training compute <a
                            href="https://news.microsoft.com/source/features/innovation/openai-azure-supercomputer/">resources</a>.
                    </p>
                    <p>The AI lab had only recently closed out its most compute-intensive project yet by that point, <a
                            href="https://cdn.openai.com/papers/gpt-4.pdf">completing</a> pre-training of GPT-4 back in
                        August 2022. Further work, of course, involved getting this new powerful model ready for public
                        use prior to its availability through the API and its consumer-facing applications by <a
                            href="https://openai.com/index/gpt-4/">March 2023</a> through deeper RLHF and red-teaming to
                        make sure the new model didn't appear unhinged. This was a step apparently skipped by <a
                            href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html">Microsoft's
                            Bing</a> who appeared to use a less polished checkpoint of the <a
                            href="https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%80%99s-GPT-4#:~:text=confirm%20that%20the%20new%20Bing%20is%20running%20on%20GPT%2D4">model</a>
                        before OpenAI officially released its version.</p>
                    <h3>The AI community and popular culture briefly lost its mind during the first half of 2023</h3>
                    <p>GPT-4 caused a splash. Microsoft's own <a href="https://arxiv.org/pdf/2303.12712">research</a>
                        described "Sparks of Artificial General Intelligence", while tech influencers like Elon signed
                        an open <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">letter</a>
                        calling to "Pause Giant AI Experiments". Others even suggesting <a
                            href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/#:~:text=willing%20to%20run%20some%20risk%20of%20nuclear%20exchange%20if%20that%E2%80%99s%20what%20it%20takes%20to%20reduce%20the%20risk%20of%20large%20AI%20training%20runs">nuking</a>
                        datacentres. The mania fizzled out quickly. Developers realised GPT-4 still couldn't follow
                        instructions reliably and made stuff up <em>all the time</em>. While the code it wrote was often
                        functional, especially for well-defined, modular functions, subtle bugs meant its use in
                        production was limited to creating tests and brainstorming. Useful, but not revolutionary - but
                        offering developers a glimpse at an agent-based future via the (at the time) wildly popular <a
                            href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>. It was also hideously
                        slow and expensive, owing to its <a
                            href="https://developer.nvidia.com/blog/nvidia-gb200-nvl72-delivers-trillion-parameter-llm-training-and-real-time-inference/#:~:text=large%20language%20models%20like%20GPT%2DMoE%2D1.8T">rumoured</a>
                        1.8 trillion parameters (albeit in an inference-efficient <a
                            href="https://arxiv.org/pdf/2401.04088">MoE</a>).</p>
                    <h3>Meta's LLaMA (Llama) was the first <em>decent</em> – and available – local LLM</h3>
                    <p>In the midst of the pundit wars surrounding Bing Chat and GPT-4, our lord and saviour Zuck saw an
                        opportunity to distract from the <a
                            href="https://www.ft.com/content/d39db54e-e1a6-4938-9b5c-b0bd41a41907">difficulties</a> with
                        his Metaverse rollout by releasing to researchers (and quickly <a
                            href="https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model-llama-leak-online-misuse">spreading</a>
                        to the wider community) a set of "Open and Efficient" foundation models – <em>Large Language
                            Model Meta AI</em> (<a
                            href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/#:~:text=LLaMA%20(Large%20Language%20Model%20Meta%20AI)%2C">LLaMA</a>).
                        Originally released as 7B, 13B, 33B and 65B parameter models without instruction fine-tuning,
                        the AI community particularly jumped on the 7B variant owing to its ability back then to run on
                        a
                        single GPU. The <a href="https://old.reddit.com/r/LocalLLaMA/">r/LocalLLaMA</a> community on
                        Reddit exploded after students at Stanford spent just a few hundred dollars to arrive at a
                        somewhat functional instruction-tuned version of Llama-1-7B, naming it <a
                            href="https://crfm.stanford.edu/2023/03/13/alpaca.html#:~:text=fine%2Dtuned%20from%20the%20LLaMA%207B%20model">Alpaca-7B</a>.
                        The model ran on my little 16GB MacBook Air M2, albeit at a modest half-token-per-second, using
                        Georgi Gerganov's <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> library. This
                        is a CPU, mind you. GPU inference was much faster thanks to Meta's very own <a
                            href="https://ai.meta.com/blog/pytorch-foundation/">PyTorch</a> ML library with
                        out-of-the-box CUDA wrappers – and the white-hot spotlight meant <em>a lot</em> of engineers
                        were looking for ways to
                        improve throughput.</p>
                    <p>Meta sturdied its <a href="https://about.fb.com/news/2023/07/llama-2/">release</a> techniques and
                        <a href="https://ai.meta.com/llama/license/">licenses</a> with Llama-2 in July 2023, complete
                        with both <a href="https://huggingface.co/meta-llama/Llama-2-7b">base</a> models and <a
                            href="https://huggingface.co/meta-llama/Llama-2-7b-chat">instruction</a>-tuned (aka chat)
                        models, making them available for commercial use, so long as it isn't available to more than
                        700 million users (unless you're like Satya and <a
                            href="https://blogs.microsoft.com/blog/2023/07/18/microsoft-and-meta-expand-their-ai-partnership-with-llama-2-on-azure-and-windows/">asked</a>
                        Zuck nicely).
                    </p>
                    <h3>The floodgates opened for useful open-weight LLMs</h3>
                    <p><a href="https://old.reddit.com/r/LocalLLaMA/">r/LocalLLaMa</a>, X/Twitter and its variants
                        feverishly egged on the community of hobbyists, academics and startups as they pumped out
                        performance <a
                            href="https://old.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/">improvements</a>
                        and fine-tunes of Llama-2.</p>
                    <p>Enter French start-up Mistral, made up of former Meta and Google researchers, having <a
                            href="https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251">raised</a> €105mn in
                        June 2023, introducing its Llama-2 competitor in September – <a
                            href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a>. This was the LLM I
                        personally used in a tonne of projects over the subsequent months, first as replacement reranker
                        for BAII's <a href="https://huggingface.co/BAAI/bge-reranker-base/tree/main">BGE</a> for my
                        locally-hosted search engine – and then in my longer-term project spanning local and cloud
                        compute. This project, under wraps for
                        now, involves trying to do what every database, DataOps or big data platform vendor (and <a
                            href="https://theintercept.com/2015/07/02/look-under-hood-xkeyscore/">government</a>) has
                        been trying to do for forever: Make everything searchable and self-explaining from any
                        perspective. I've been working on aspects of this project for years, both personally and
                        professionally – full essay (and much, much more) coming <a
                            href="https://old.reddit.com/r/etymology/comments/6g8pk8/how_did_the_coming_soontm_thing_start/">soon</a>.
                    </p>
                    <p>All of the above, however, still weren't quite robust enough to use in customer-facing
                        applications – or even during software demos (I won't share a link here – we've all seen them).
                        Llama-2-70B rivalled GPT-3.5-turbo in most benchmarks, but neither exhibited anything resembling
                        <em>effective</em> agency. When faced with real-world data, I'd end up filling the 8k context
                        window with instructions on how to handle edge cases, or worse – bulk out our inference
                        cluster with trees of fine-tunes or <a href="https://arxiv.org/pdf/2106.09685">LoRAs</a> to
                        facilitate multi-step, multi-persona verification. Plus, these expensive instances offering
                        obscene amounts of VRAM, critical to the proper functioning of "AI-powered" applications, are
                        sitting idle during off-peak hours, with cold-start times in the minutes (more on this later).
                    </p>
                    <p>Assuming they're up and running, what's the experience for the end user? Sitting around for an
                        (unknown) number of minutes while a flock of hapless stochastic parrots get lost in the noisy
                        details. <a
                            href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/tree/main">Quantisation</a>
                        definitely speeds things up, but the performance degradation is definitely noticeable and far
                        less predictable.</p>
                    <h3>Mixtral-of-Experts takes enterprise self-hosted LLMs mainstream</h3>
                    <p>Christmas is one of my favourite holidays. Not just taking a break and spending time with family,
                        but also (usually) having at least a week away from work-related product development, client
                        product workshops and deadlines. Also getting time to step back and put some time into tinkering
                        – whether that's reconditioning a vintage searchlight (post on this coming soon) or testing out
                        Mistral's new (in December 2023) <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral
                            8x7B</a>, a sparse mixture-of-experts LLM claiming to outperform Llama-2-70B and
                        GPT-3.5-turbo.</p>
                    <p>Running Mixtral wasn't possible on my little MacBook – but it most certainly was on the
                        budget-friendly NVIDIA <a href="https://www.nvidia.com/en-gb/data-center/a40/">A40s</a> offered
                        by most cloud GPU platforms. They're not quite ancient, but certainly not current-gen. It
                        launched back in 2020 alongside the A100 (of GPT-4 fame). With 48GB of VRAM each, I spun up four
                        A40s
                        and waited 30 minutes for the 100GB weights to download. This was around the time the <a
                            href="https://github.com/vllm-project/vllm">vLLM</a> library was gaining a strong track <a
                            href="https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit">record</a>
                        in production applications – and quickly added <a
                            href="https://github.com/vllm-project/vllm/pull/2011">support</a> for Mixtral-8x7B.</p>
                    <p>Even on older hardware, inferencing was FAST – delivering reading-speed throughput for each
                        stream up to at least 150 concurrency. MoE really is built for production inferencing,
                        activating only a fraction of parameters with each forward pass and squeezing
                        every FLOP out of each GPU you're paying by the hour for.</p>
                    <h3>Making a strong, self-hosted LLM available 24/7/365 is <strong>expensive</strong></h3>
                    <p>But Mixtral running in the cloud wasn't a local LLM anymore. Sure, it's running on infrastructure
                        you have more control over than a random endpoint sold through DMs on Reddit, but it's also damn
                        expensive. Around $4/hour running on four A40s if you need it to be available with sub-second
                        latency, 24/7/365. You basically can't run it on a CPU (otherwise we're talking seconds per
                        token, not tokens per second). Hence, we're forced to keep the model sitting in VRAM, spending
                        at least $100/day, multiplied by availability regions. This is fine if you're serving thousands
                        of users from your clusters – even briefly spinning up additional instances equipped with H100s
                        to manage traffic spikes through <a href="https://kubernetes.io/">Kubernetes</a> or EC2 <a
                            href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html">Auto
                            Scaling Groups</a>.</p>
                    <p>But what if you're a smaller firm, employing a few dozen people using Mixtral to improve search,
                        tagging and data structuring? What if your clusters are sitting idle at night and at weekends?
                        Do you only spin them up at certain times of the day? Otherwise you're spending $3000 per month
                        on inference capacity for a model that barely breaks a sweat at
                        peak times.</p>
                    <p>This is why almost everyone else uses a pay-per-token inference endpoint like Mistral <a
                            href="https://mistral.ai/news/mistral-large-2407/">Large</a> on <a
                            href="https://mistral.ai/technology/#models">La Platforme</a> or Microsoft <a
                            href="https://azure.microsoft.com/en-us/blog/microsoft-and-mistral-ai-announce-new-partnership-to-accelerate-ai-innovation-and-introduce-mistral-large-first-on-azure/">Azure</a>,
                        <a href="https://azure.microsoft.com/en-gb/products/ai-services/openai-service">GPT-4o</a>
                        through Azure OpenAI Service, <a href="https://aws.amazon.com/bedrock/claude/">Claude</a>
                        through <a href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a> or any of the open-source
                        models on cloud <a href="https://groq.com/">platforms</a> – even Hugging Face offers <a
                            href="https://huggingface.co/blog/llama3#inference-integrations">endpoints</a> for
                        inferencing popular models. The problem, of course, is that LLM inferencing by these services
                        fundamentally can't be done without having unencrypted data from multiple clients passing
                        through the system at the same time. But who cares? Most companies already trust Microsoft 365,
                        Google Workspace and even Dropbox with their sensitive data, barely giving a thought to whether
                        vendors properly <a
                            href="https://arstechnica.com/security/2023/09/hack-of-a-microsoft-corporate-account-led-to-azure-breach-by-chinese-hackers/">secure</a>
                        their tech stack and <a
                            href="https://en.wikipedia.org/wiki/2020_United_States_federal_government_data_breach">supply
                            chain</a>.
                    </p>
                    <p>For truly sensitive data, those consistently targeted the most by sophisticated threats –
                        critical infrastructure, spy agencies, military contractors – the only real options are
                        on-premises
                        deployment or some of the hardened offerings from <a
                            href="https://www.palantir.com/platforms/gotham/">Palantir</a> or directly from hyperscalers
                        like AWS <a href="https://aws.amazon.com/govcloud-us/">GovCloud</a>, Azure for <a
                            href="https://azure.microsoft.com/en-us/explore/global-infrastructure/government">Government</a>
                        or Google's <a href="https://cloud.google.com/gov/federal-defense-and-intel">US DoD</a>.</p>
                    <p>For those who can't <a
                            href="https://assets.applytosupply.digitalmarketplace.service.gov.uk/g-cloud-14/documents/92736/804537709233305-pricing-document-2024-05-01-1339.pdf">afford</a>
                        government-oriented offerings, to be totally sure API endpoints aren't <a
                            href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/content-filter">holding</a>
                        onto our sensitive data for mercurial reasons, we're forced to spin up a privately-managed
                        cluster. This could be on AWS EC2 or Azure <a
                            href="https://azure.microsoft.com/en-gb/products/virtual-machines/">VM</a>, where developers
                        are given the tools (assuming they have the <a
                            href="https://arstechnica.com/cars/2024/12/whistleblower-finds-unencrypted-location-data-for-800000-vw-evs/">skills</a>)
                        to build something pretty close to bulletproof from a cybersecurity and data
                        privacy point of view. There's also the option of purchasing some <a
                            href="https://www.ebay.co.uk/sch/i.html?_from=R40&amp;_nkw=h100+gpu&amp;_sacat=0&amp;rt=nc&amp;LH_ItemCondition=4">used</a>
                        H100s for $40k a pop and run your inferencing on-premises.</p>
                    <p>Once we start talking about running larger models like DeepSeek-V3 with 671B parameters – even
                        with FP8 mixed precision inference and only 37B <a
                            href="https://github.com/deepseek-ai/DeepSeek-V3#:~:text=37B%20activated%20for%20each%20token">activated</a>
                        with every forward pass – we'll need at least a couple dozen A40s. </p>
                    <p>But what if we could get powerful models to run on more modest hardware? This quite simply lowers
                        the bar for getting low-latency access to useful LLMs without the financial burden of
                        keeping hundreds of billions of parameters ready and waiting in VRAM.</p>
                    <p>Enter, also in late December 2023, Alibaba Cloud's <a href="https://huggingface.co/Qwen">Qwen</a>
                        – from the Chinese 通义千问 (Tongyi Qianwen) meaning "universal understanding, thousand questions".
                        The team released the weights to models like <a
                            href="https://huggingface.co/Qwen/Qwen1.5-7B">Qwen1.5-7B</a> and <a
                            href="https://huggingface.co/Qwen/Qwen2-7B-Instruct">Qwen2-7B-Instruct</a> – but really
                        gained attention after releasing their <a
                            href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5</a> model in September, right
                        around the time a bunch of their team was <a
                            href="https://www.scmp.com/tech/big-tech/article/3283876/chinese-tech-firms-scramble-recruit-top-ai-talent-amid-short-supply">poached</a>
                        by ByteDance.</p>
                    <h3>Running a strong LLM locally is a <strong>very</strong> recent phenomenon</h3>
                    <p>Qwen's 7B model was a <a href="https://qwenlm.github.io/blog/qwen2.5/">noticeable</a> improvement
                        on prior favourites like Mistral's latest <a
                            href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">7B</a> and Meta's <a
                            href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama-3.1-8B</a> <a
                            href="https://ai.meta.com/blog/meta-llama-3-1/">released</a> in July. Despite a few in the
                        AI community <a href="https://arxiv.org/pdf/2401.05566">wondering</a> what ulterior motive could
                        be behind the provision of such a strong, free LLM (<a
                            href="https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/">Zuck?</a>), it
                        was certainly better at understanding my
                        instructions, less eager to make stuff up and – most importantly – followed those instructions
                        more reliably. For small models like this, we're not asking it to perfectly regurgitate facts or
                        be a sensitive conversationalist. We just want it to follow instructions precisely and reliably
                        raise an error when it's unsure. This is critical to backend data management, reranking, tagging
                        and adding structure to knowledge bases at enterprise scale.</p>
                    <p>The competition in the small model space continued with Meta's <a
                            href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama
                            3.2</a> in late September, shrinking all the way down to <a
                            href="https://huggingface.co/meta-llama/Llama-3.2-1B">1B</a> parameters, Mistral's "edge
                        models" <a href="https://mistral.ai/news/ministraux/">launched</a> in October with <a
                            href="https://huggingface.co/ministral/Ministral-3b-instruct">Ministral 3B</a> and <a
                            href="https://huggingface.co/mistralai/Ministral-8B-Instruct-2410">Ministral-8B</a>,
                        alongside the multimodal <a href="https://mistral.ai/news/pixtral-large/">Pixtral</a> in
                        November. </p>
                    <p>The latest little models from Qwen, Mistral and Meta were already causing developers to
                        second-guess decisions to use API endpoints for running huge batch jobs – stemmed somewhat by
                        OpenAI launching a <a
                            href="https://community.openai.com/t/batchapi-is-now-available/718416">BatchAPI</a> back in
                        April and Anthropic following with <a
                            href="https://www.anthropic.com/news/message-batches-api">Message Batches</a> for Claude in
                        October. But my guess is that it won't be enough.</p>
                    <p>Claude 3.5 Sonnet (10/2024 edition) has, according to a recent Microsoft <a
                            href="https://arxiv.org/pdf/2412.19260v1">paper</a>, around 175B parameters, GPT-4o around
                        200B and GPT-4o-mini 8B. It also mentioned o1-preview having 300B and o1-mini having 100B.
                        Distributed inference models like these are almost always MoE, like DeepSeek-V3 – such
                        architectures give model providers more performance per GPU, so long as they're fed with enough
                        traffic.
                        Whether the paper estimates the active parameters during each forward pass or the total number
                        of
                        parameters is unclear.</p>
                    <p>Aside from GPT-4o-mini's reported 8B parameters (<a
                            href="https://openai.com/api/pricing/">priced</a> at $0.075 input/$0.30 output per M from
                        the BatchAPI) – the open-weight LLMs are an order of magnitude smaller in terms of compute
                        footprint.</p>
                    <h3>Running an LLM locally for batch processing changes the story</h3>
                    <p>But what if we only need a powerful LLM for indexing our data? Let's say we're
                        cleaning/extracting structured data from imperfectly OCRed scanned documents, performing entity
                        resolution or even building an internal Wiki for a medium-sized professional services firm.
                        Strong model performance is needed, let's something like DeepSeek-V3. The tasks it
                        is performing require high reliability – but once indexed, the heavy lifting has already been
                        done.</p>
                    <p>We spin up the model a couple of times a day, it churns through a few thousand pages of text,
                        shutting down once the queue hits zero. This might be cost-effective, even for a little firm.
                        Here's why.</p>
                    <h3>Concurrent requests deliver massive overall throughput</h3>
                    <p>My various projects over the years with ML model inferencing have long been done with the
                        understanding that batched inference delivers much better overall throughput – although the
                        optimal batch size depends on model architecture and the type of task. Conventional ML models
                        are intuitive – input size is fixed and processing time only depends on model architecture,
                        hardware and batch size.
                        LLMs are interesting in that throughput depends on a combination of hardware, the number of
                        concurrent requests, the amount of input (prompt) tokens and the expected number of tokens
                        generated (which increase the effective input tokens at every generation step).</p>
                    <p>Therefore, I've devised a test. DeepSeek-V3 would be fun, although I'd prefer someone else were
                        paying to rent two dozen A40s (or
                        worse, a dozen H100s). So, the heatmap below shows Qwen-2.5-7B(-Instruct, although it doesn't
                        matter for this experiment) running via vLLM in BF16 on an NVIDIA A5000 (24GB VRAM). Additional
                        tests on an A6000 (48GB) and H100 (80GB) were also performed. Yes, Qwen2.5 is a
                        dense model rather than DeepSeek's MoE – but the learning outcomes are the same.</p>
                    <p>The experiment began with creating a system prompt guaranteed to generate at least 128 tokens, at
                        which point vLLM would stop generation:
                    </p>
                    <pre><code class="language-python">system_message = """You are a diligent codebreaking assistant.
I want you to carefully analyse the query provided by the user, which might look like a string of random characters.
However, this isn't actually the case. This string contains a hidden message that I need you to try to decipher.
You should begin by carefully discussing what features exist in the string, before providing step-by-step instructions on how to detect patterns related to them."""</code></pre>
                    <p>Then onto generating random strings after the system message to meet the target input (prompt)
                        token count for each batch
                        using simple binary search and a tonne of parallel requests to the <code>/tokenize</code>[sic]
                        endpoint:</p>
                    <pre><code class="language-python">def generate_random_query(min_length, max_length):
    length = random.randint(min_length, max_length)
    characters = string.ascii_letters + string.digits + ' '
    return ''.join(random.choice(characters) for _ in range(length))

def format_chat_prompt(messages):
    formatted = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        formatted += f"<|im_start|>{role}\n{content}<|im_end|>\n"
    return formatted

def generate_tokenised_queries(
    system_message: str,
    target_total_tokens: list[int],
    queries_per_target: list[int],
    tokeniser_base_url: str = "http://localhost:8000",
    max_concurrent: int = 50
) -> dict[int, list[str]]:
    formatted_prompt = format_chat_prompt([{"role": "system", "content": system_message}])
    response = requests.post(
        f"{tokeniser_base_url}/tokenize",
        json={
            "model": "Qwen/Qwen2.5-7B-Instruct",
            "prompt": formatted_prompt
        }
    )
    system_tokens = len(response.json()["tokens"])
    
    token_char_ratios = []
    
    def update_ratio(chars: int, tokens: int):
        ratio = chars / tokens
        token_char_ratios.append(ratio)
        if len(token_char_ratios) > 100:  # last 100 samples
            token_char_ratios.pop(0)
        return np.mean(token_char_ratios)
    
    def get_token_count(text: str) -> int:
        formatted_prompt = format_chat_prompt([{"role": "user", "content": text}])
        response = requests.post(
            f"{tokeniser_base_url}/tokenize",
            json={
                "model": "Qwen/Qwen2.5-7B-Instruct",
                "prompt": formatted_prompt
            }
        )
        tokens = len(response.json()["tokens"])
        update_ratio(len(text), tokens)
        return tokens
    
    def binary_search_length(target_tokens: int, min_length: int = 10, max_length: int = 200) -> tuple[int, int]:
        min_length = min(min_length, target_tokens // 4)  # estimate based on tokens/chars ratio
        max_length = max(max_length, target_tokens)  # start target tokens as max
        
        min_text = generate_random_query(min_length, min_length)
        min_tokens = get_token_count(min_text)
        max_text = generate_random_query(max_length, max_length)
        max_tokens = get_token_count(max_text)
        
        while min_tokens > target_tokens:
            min_length = max(1, min_length // 2)
            min_text = generate_random_query(min_length, min_length)
            min_tokens = get_token_count(min_text)
            
        while max_tokens < target_tokens:
            max_length *= 2
            max_text = generate_random_query(max_length, max_length)
            max_tokens = get_token_count(max_text)
        
        while max_length - min_length > 5:
            mid_length = (min_length + max_length) // 2
            mid_text = generate_random_query(mid_length, mid_length)
            mid_tokens = get_token_count(mid_text)
            
            if mid_tokens < target_tokens:
                min_length = mid_length
                min_tokens = mid_tokens
            else:
                max_length = mid_length
                max_tokens = mid_tokens
        
        return min_length, max_length
    
    queries_dict = {}
    
    for target, num_queries in zip(target_total_tokens, queries_per_target):
        print(f"\nGenerating {num_queries} queries for {target} tokens...")
        target_prompt_tokens = target - system_tokens
        queries = set()
        
        min_length, max_length = binary_search_length(target_prompt_tokens)
        print(f"Found length range: {min_length} - {max_length}")
        
        with tqdm(total=num_queries, desc=f"Tokens: {target}") as pbar:
            while len(queries) < num_queries:
                # increase batch size for higher token counts (this doesn't always work)
                remaining = num_queries - len(queries)
                batch_multiplier = 4 if target_prompt_tokens > 512 else 2
                batch_size = min(max_concurrent, remaining * batch_multiplier)
                
                # token/char ratio estimate text length
                avg_ratio = np.mean(token_char_ratios) if token_char_ratios else 4.0
                estimated_chars = int(target_prompt_tokens * avg_ratio)
                estimated_length = min(max_length, max(min_length, estimated_chars))
                
                candidates = [
                    generate_random_query(
                        max(min_length, estimated_length - 5),
                        min(max_length, estimated_length + 5)
                    )
                    for _ in range(batch_size)
                ]
                
                with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                    futures = {
                        executor.submit(get_token_count, c): c 
                        for c in candidates
                    }
                    
                    for future in as_completed(futures):
                        if len(queries) >= num_queries:
                            for f in futures:
                                if not f.done():
                                    f.cancel()
                            break
                            
                        try:
                            tokens = future.result()
                            text = futures[future]
                            
                            # acceptable token range (within 1% of target)
                            token_tolerance = max(1, int(target_prompt_tokens * 0.01))
                            min_acceptable = target_prompt_tokens - token_tolerance
                            max_acceptable = target_prompt_tokens + token_tolerance
                            
                            if min_acceptable <= tokens <= max_acceptable:
                                if text not in queries:
                                    queries.add(text)
                                    pbar.update(1)
                                    
                                    if len(queries) >= num_queries:
                                        break
                                        
                        except Exception as e:
                            print(f"it broke: {e}")
                            continue
        
        queries_dict[target] = list(queries)[:num_queries]
    
    return queries_dict</code></pre>
                    <p>Finally, the parametric sweep:</p>
                    <pre><code class="language-python">def run_parameter_sweep(
    batch_sweep,
    target_tokens,
    system_message,
    sequential_completions=100,
    max_tokens=128,
):
    # single test query with first target token count to check it's working (you do NOT want to find this out an hour later)
    print("Test query...")
    test_queries = generate_tokenised_queries(
        system_message,
        target_total_tokens=[target_tokens[0]],
        queries_per_target=[1]
    )
    test_query = test_queries[target_tokens[0]][0]
    
    print("System message:")
    print(f"'{system_message}'")
    print("Test query:")
    print(f"'{test_query}'")
    
    chat_response = client.chat.completions.create(
        model="Qwen/Qwen2.5-7B-Instruct",
        messages=[
            {"role": "system", "content": system_message.strip()},
            {"role": "user", "content": test_query.strip()},
        ],
        temperature=0.0,
        max_tokens=max_tokens,
    )
    
    print("Model out:")
    print(f"'{chat_response.choices[0].message.content}'")
    print(f"Prompt tokens: {chat_response.usage.prompt_tokens}")
    print(f"Completion tokens: {chat_response.usage.completion_tokens}")
    print(f"Total tokens: {chat_response.usage.total_tokens}")
    
    heatmap_data = np.zeros((len(batch_sweep), len(target_tokens)))
    latency_data = np.zeros((len(batch_sweep), len(target_tokens)))

    for i, batch_size in enumerate(batch_sweep):
        for j, target in enumerate(target_tokens):
            print(f"Testing batch size {batch_size}, target tokens {target}...")
            
            # fresh queries for this specific test to avoid cache
            queries_dict = generate_tokenised_queries(
                system_message,
                [target],
                [batch_size * sequential_completions]
            )
            queries = queries_dict[target]
            
            total_tokens = 0
            completion_times = []  # individual completion times
            start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=batch_size) as executor:
                futures = [
                    executor.submit(
                        send_request,
                        client,
                        system_message,
                        query,
                        max_tokens,
                    )
                    for query in queries
                ]
                
                for future in as_completed(futures):
                    try:
                        latency, tokens_used, _ = future.result()
                        total_tokens += tokens_used
                        completion_times.append(latency)
                    except Exception as e:
                        print(f"Error: {e}")
            
            total_time = time.time() - start_time
            avg_latency = statistics.mean(completion_times)
            throughput = total_tokens / total_time
            
            print(f"Average latency: {avg_latency:.2f}s")
            print(f"Total throughput: {throughput:.2f} tokens/s")
            heatmap_data[i, j] = throughput
            latency_data[i, j] = avg_latency
    
    plot_heatmap(heatmap_data, latency_data, batch_sweep, target_tokens)</code></pre>
    <p>What we're doing with the above is basically:</p>
                    <ol>
                        <li>Query generation:
                            <ul>
                                <li>Creates random text queries of specific lengths (chars)</li>
                                <li>Uses a binary search algorithm to find queries that will result in exact target
                                    token counts</li>
                                <li>Ensures each query (excluding system message) is unique and meets the desired token
                                    length requirements (we don't want to hit that prefix cache and skew our results)</li>
                                <li>This is performed before each test to ensure the server is kept fed with a precise
                                    number of workers (via <a
                                        href="https://docs.python.org/3/library/concurrent.futures.html">ThreadPoolExecutor</a>)
                                </li>
                            </ul>
                        </li>
                        <li>Performance testing:
                            <ul>
                                <li>Different combinations of:
                                    <ul>
                                        <li>Concurrent requests (how many queries are sent simultaneously)</li>
                                        <li>Input token counts (how long each query is, including the system prompt,
                                            which is static at 120 tokens)</li>
                                    </ul>
                                </li>
                                <li>Output tokens are limited to 128 – the system prompt ensures the model always meets
                                    this limit before stopping generation</li>
                                <li>Measures the server's throughput (tokens processed per second) for each combination
                                </li>
                                <li>vLLM delivers continuous batching, so the results represent the average throughput
                                    at each concurrency level for completing a set total number of requests (e.g. 1,000
                                    total completions at a batch size of 10 and 15,000 total completions at a batch size of
                                    150)</li>
                            </ul>
                        </li>
                    </ol>
                    <p>The result of this experiment is shown below:</p>
                    <h3>Overall throughput with batch size and input (prompt) token count</h3>
                    <div class="post-image-wrapper">
                        <img src="/images/throughput_A5000_Qwen25-7B.png" alt="Concurrency Cover" loading="lazy" width="768"
                            height="727" onload="this.classList.add('loaded')" class="post-image">
                    </div>
                    <p class="image-caption">Throughput measured with NVIDIA A5000 (24GB VRAM) with Qwen2.5-7B-Instruct</p>
                    <p>Clearly, sequential processing is <strong>slow</strong> when we're trying to maximise overall
                        throughput for batch jobs. We're talking 45 tokens per second with a short, 128-token input –
                        and down to 40 when inputting a more substantial 2048 tokens (the context limit for
                        February 2023's Llama-1, if you'd believe it).</p>
                    <p>As soon as we're pushing more parallel requests through this A5000 server, results look more
                        promising. At a concurrency of 10, overall throughput hits 389 per second for the short prompt
                        and 191 for the long prompt. With 50 parallel requests, we're seeing a much larger difference between the short and long prompts, with 1196 and 256 respectively. The peak
                        overall throughput occurs with the shortest prompt and 120 concurrent requests, at a massive
                        <strong>1821 tokens per second</strong>. Not bad for a GPU instance that costs $0.40/hr to rent?
                        With longer input lengths, however, performance definitely suffers – delivering a peak
                        throughput of 246 tokens per second with 50 concurrent requests. Once we start redlining the VRAM, vLLM just queues up surplus requests and performance plateaus.
                    </p>
                    <p>But does increasing VRAM make a difference? The more mighty A6000 GPU, with 48GB VRAM, didn't fare much better – also plateuing around 1950 tokens per second after 150 concurrent requests with 128 input tokens.
                        What about the holy grail, the H100 SXM? A whole generation newer, a whopping 80GB VRAM, surely it'll blow the A5000 out of the water?
                        To be clear, it does – delivering just over <strong>5000 tokens per second</strong> with 128 input tokens.
                        Even with 2048 input tokens, the H100 pumps out around 1600 Qwen2.5-7B-Instruct tokens per second.
                        But at $3/hr? The little A5000, at $0.40/hr, is better value for money for both short and long input token use cases. The insane 900GB/s memory bandwidth of the H100 is just wasted on such a small, dense model.
                    </p>
                    <p>But let's compare our Qwen-2.5-7B-Instruct on the A5000 with OpenAI's cheapest closed model, GPT-4o-mini, this time served from <code>api.openai.com</code>
                        or their Azure OpenAI Service wrapper. As mentioned in their pricing
                        <a href="https://openai.com/api/pricing/">page</a>, requests through the BatchAPI are $0.075 per
                        million input (prompt) tokens and $0.30 per million output tokens. If we have one million tasks
                        with around 128 prompt tokens and 128 completion (output) tokens on average, it'll cost (1M x
                        128 x $0.075) + (1M x 128 x $0.30) = $48. If we have 2048 input tokens per task, the cost
                        shoots up to $192 for the 1M batch.
                    </p>
                    <p>The A5000 instance hosting Qwen-2.5-7B-Instruct? For the 128 token input use case, it's
                        completing about 1800 tokens per second – so a single A5000 instance will take
                        about 20 hours to churn through all one million tasks. If we want it done faster, we
                        can simply scale up the number of A5000 inference nodes (or go for a more powerful GPU). At
                        $0.40/hr for a single A5000 instance, that's $8 to complete the 1M batch. For the 2048 input
                        token use case, we're talking $60 to blast through the 1M. Big
                        saving, assuming we get GPT-4o-mini performance out of Qwen-2.5-7B-Instruct.
                    </p>
                    <p>And yes, I've found in testing over the past couple of months that Qwen delivers pretty
                        similar reliability in medium-intelligence tag/categorise/etc. enterprise data processing tasks
                        compared to GPT-4o-mini.
                    </p>
                    <p>One more aspect to consider with concurrent LLM inferencing is the per-request latency.
                        While vLLM does a great job at continuous batching, it does, however, obviously result in almost linear increases in latency as the number of concurrent requests and input token counts increase.
                        The chart below shows how latency varies with concurrency and input token count for the A5000 instance.
                    </p>
                    <h3>Overall latency with batch size and input (prompt) token count</h3>
                    <div class="post-image-wrapper">
                        <img src="/images/latency_A5000_Qwen25-7B.png" alt="Concurrency Cover" loading="lazy" width="768"
                            height="742" onload="this.classList.add('loaded')" class="post-image">
                    </div>
                    <p class="image-caption">Latency measured with NVIDIA A5000 (24GB VRAM) with Qwen2.5-7B-Instruct</p>
                    <p>When the server is close to idle, the time to complete 128 tokens is around 3 seconds, regardless of input token count.
                        At our 120 concurrency and 128 input token count, the per-request latency is around around 8 seconds.
                        Not too bad – but once we start redlining VRAM, we're talking 30 seconds, 60 seconds, all the way to 73 seconds for the 150 concurrency/2048 input scenario.
                        While this isn't an issue for batch processing, it's a dealbreaker for consumer and enterprise applications.
                        For reference, the H100 SXM fares much better here – delivering a per-request latency of 12 seconds for the 150/2048 scenario.
                        Even if we want to keep latency to around 5 seconds, we're still beating GPT-4o-mini with the A5000 and Qwen-2.5-7B-Instruct on price.
                    </p>
                    <h3>Compute-heavy models like DeepSeek-V3 are economically viable if you're strictly batch
                        processing</h3>
                    <p>Back to our original question. At the scale of much larger models like DeepSeek-V3, small firms
                        might not be able to run them
                        all day, every day. But there's definitely the business case for running them for <em>super
                            valuable</em> backend data management tasks like tagging, entity resolution or building
                        wikis – and even populating something that looks a bit like a knowledge graph. Just make sure
                        you're starting up and shutting down fast and keeping it as saturated as possible while online.
                    </p>
                    <p>So yes, every organisation can't have a truly private DeepSeek-V3 instance available all day, every day. But
                        we're getting there fast – <em>Small LLMs are getting good. And giant LLMs are getting cheap</em>. In the meantime, we can run such models as intermittent batch
                        processors. All without sending our data to people we know will try to sell it back to us one
                        day.</p>
                </div>

                <footer class="post-footer">
                    <div class="post-navigation">
                        <a href="/years/2024.html" class="back-to-posts" hx-get="/years/2024.html"
                            hx-select="#main-content" hx-target="#main-content" hx-push-url="true">← Back to 2024
                            posts</a>
                    </div>
                </footer>
            </article>
        </main>
        <div id="footer-container" hx-get="/components/footer.html" hx-trigger="load" hx-swap="innerHTML">
            <div class="htmx-indicator">Footer hasn't loaded yet...</div>
        </div>
    </div>
</body>

</html>
